# v2.2 Next Steps & Roadmap

> **Status update (2025-11-05)**: Multi-extractor progressive streaming is not yet wired into the backend. Treat the tasks below as future work items rather than already delivered functionality.

**Current Status**: ‚úÖ Architecture Complete, Implementation Ready
**Date**: November 4, 2025

---

## What's Done ‚úÖ

1. **Multi-Extractor Ensemble System** - Production ready
2. **Async Progressive Extraction** - WebSocket streaming implemented
3. **Cost Management & Budget Limits** - Budget enforcement ready
4. **Weighted Voting & Consensus** - Cross-validation algorithm complete
5. **Comprehensive Documentation** - 1,820 lines of guides and examples

---

## Immediate Next Steps (This Week)

### 1. Test the Implementation üß™

**Status**: Ready to test
**Files**: `examples/test_progressive_extraction.py` (created)

#### Steps:

```bash
# Terminal 1: Start backend
cd backend
source ../.venv/bin/activate
uvicorn main:app --reload

# Terminal 2: Test progressive extraction
cd ..
python examples/test_progressive_extraction.py assets/reference_images/retro-audio-ui.png
```

**Expected Output**:
```
üîå Connecting to ws://localhost:8000/api/v1/extract/progressive...
‚úÖ Connected!

üì§ Uploading 1 image(s)...
   ‚úì retro-audio-ui.png (45,123 bytes)

‚è≥ Extraction started...

üìä Result #1 (0.32s)
   Stage: cv_complete
   Tier: 1
   CV Time: 0.320s
   Total Cost: $0.0000
   Palette (8 colors):
      primary: #F15925 (conf: 0.75)
      secondary: #1C5D6B (conf: 0.75)
      neutral: #E8E8E8 (conf: 0.75)

‚úÖ Connection closed by server
üéâ Completed! Total time: 0.35s
   Received 1 progressive result(s)
```

**Current Limitation**: Only CV extraction (Tier 1) works. AI extractors not yet integrated.

---

### 2. Add Unit Tests üß™

**Priority**: HIGH
**Estimate**: 2-3 hours

Create tests for the multi-extractor system:

```python
# extractors/tests/test_multi_extractor.py

import pytest
from extractors.ai.multi_extractor import (
    MultiExtractor,
    ExtractorConfig,
    ExtractorTier
)

@pytest.mark.asyncio
async def test_progressive_extraction():
    """Test progressive streaming of results."""

    def mock_fast(imgs):
        return {"palette": {"primary": "#F15925"}}

    def mock_slow(imgs):
        return {"palette": {"primary": {"hex": "#F15925", "name": "molten-copper"}}}

    extractors = [
        ExtractorConfig("fast", mock_fast, ExtractorTier.FAST, weight=1.0),
        ExtractorConfig("slow", mock_slow, ExtractorTier.SLOW, weight=1.2),
    ]

    multi = MultiExtractor(extractors)
    results = []

    async for result in multi.extract_progressive([mock_image]):
        results.append(result)

    assert len(results) == 2  # 2 tiers
    assert results[0]["_metadata"]["tier"] == 1
    assert results[1]["_metadata"]["tier"] == 3


@pytest.mark.asyncio
async def test_budget_enforcement():
    """Test cost budget enforcement."""

    extractors = [
        ExtractorConfig("free", free_fn, ExtractorTier.FAST, cost=0.0),
        ExtractorConfig("expensive", expensive_fn, ExtractorTier.SLOW, cost=0.10),
    ]

    multi = MultiExtractor(extractors, max_cost=0.05)
    results = []

    async for result in multi.extract_progressive([mock_image]):
        results.append(result)

    # Should only run Tier 1 (expensive tier skipped)
    assert len(results) == 1
    assert results[0]["_metadata"]["total_cost"] == 0.0


@pytest.mark.asyncio
async def test_weighted_voting():
    """Test consensus building with weighted votes."""

    def extractor1(imgs):
        return {"palette": {"primary": "#F15925"}}

    def extractor2(imgs):
        return {"palette": {"primary": "#F25A27"}}  # Slightly different

    def extractor3(imgs):
        return {"palette": {"primary": "#F15925"}}  # Agrees with extractor1

    extractors = [
        ExtractorConfig("e1", extractor1, ExtractorTier.FAST, weight=1.0),
        ExtractorConfig("e2", extractor2, ExtractorTier.FAST, weight=0.9),
        ExtractorConfig("e3", extractor3, ExtractorTier.FAST, weight=1.1),
    ]

    multi = MultiExtractor(extractors, enable_cross_validation=True)

    async for result in multi.extract_progressive([mock_image]):
        palette = result["palette"]["primary"]

        # Should choose #F15925 (2.1 weight) over #F25A27 (0.9 weight)
        assert palette["hex"] == "#F15925"
        assert palette["confidence"] > 0.7
        assert palette["votes"] == 3
```

**Run tests**:
```bash
cd extractors
../.venv/bin/pytest tests/test_multi_extractor.py -v
```

---

### 3. Integrate AI Extractors ü§ñ

**Priority**: HIGH
**Estimate**: 3-4 hours

Add GPT-4 Vision extractor to the ensemble:

#### Step 1: Create GPT-4 Vision Wrapper

```python
# extractors/extractors/ai/gpt4_vision_extractor.py (update existing)

from typing import List, Dict, Any
import numpy as np
from PIL import Image
import base64
import io
import os

class GPT4VisionExtractor:
    """Extract design tokens using GPT-4 Vision API."""

    def __init__(self):
        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY not set")

    def extract(self, images: List[np.ndarray]) -> Dict[str, Any]:
        """Extract tokens from images using GPT-4 Vision."""
        import openai

        # Convert first image to base64
        img = Image.fromarray(images[0])
        buffer = io.BytesIO()
        img.save(buffer, format="PNG")
        img_base64 = base64.b64encode(buffer.getvalue()).decode()

        # Call GPT-4 Vision API
        response = openai.ChatCompletion.create(
            model="gpt-4-vision-preview",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": """Analyze this UI design and extract:
                            1. Primary color (hex code + semantic name)
                            2. Secondary color (hex code + semantic name)
                            3. Accent color (hex code + semantic name)

                            Return JSON: {"palette": {"primary": {"hex": "...", "name": "..."}, ...}}"""
                        },
                        {
                            "type": "image_url",
                            "image_url": f"data:image/png;base64,{img_base64}"
                        }
                    ]
                }
            ],
            max_tokens=500
        )

        # Parse response
        result_text = response.choices[0].message.content
        # Extract JSON from response...

        return {"palette": {...}}  # Parsed result
```

#### Step 2: Add to Multi-Extractor Configuration

```python
# backend/routers/extraction.py (update)

from extractors.ai.gpt4_vision_extractor import GPT4VisionExtractor

# Configure extractors
extractors = [
    # Tier 1: Fast CV (required)
    ExtractorConfig(
        name="opencv_color",
        extract_fn=lambda imgs: _extract_tokens(imgs, lambda p: None),
        tier=ExtractorTier.FAST,
        weight=1.0,
        required=True
    ),

    # Tier 3: GPT-4 Vision (optional)
    ExtractorConfig(
        name="gpt4_vision",
        extract_fn=GPT4VisionExtractor().extract,
        tier=ExtractorTier.SLOW,
        weight=1.2,
        cost_per_call=0.02,
        enabled=bool(os.getenv("OPENAI_API_KEY"))
    ),
]

multi = MultiExtractor(extractors, max_cost=0.10)
```

---

### 4. Build Frontend Components üé®

**Priority**: MEDIUM
**Estimate**: 4-6 hours

Create React components for progressive streaming:

```typescript
// frontend/src/components/ProgressiveExtractor.tsx

import { useState, useEffect } from 'react';
import { ProgressBar } from './ProgressBar';
import { ConfidenceBadge } from './ConfidenceBadge';
import { TokenPreview } from './TokenPreview';

interface ProgressiveResult {
    stage: string;
    tier: number;
    tokens: any;
    confidence: number;
    elapsed_ms: number;
    total_cost: number;
}

export function ProgressiveExtractor() {
    const [results, setResults] = useState<ProgressiveResult[]>([]);
    const [status, setStatus] = useState<string>('idle');
    const [currentTier, setCurrentTier] = useState<number>(0);

    const handleUpload = async (files: File[]) => {
        setStatus('connecting');

        const ws = new WebSocket('ws://localhost:8000/api/v1/extract/progressive');

        ws.onopen = async () => {
            setStatus('uploading');

            // Convert to base64
            const base64Images = await Promise.all(
                files.map(file => fileToBase64(file))
            );

            // Send extraction request
            ws.send(JSON.stringify({
                action: 'extract',
                images: base64Images,
                use_ai: true
            }));

            setStatus('extracting');
        };

        ws.onmessage = (event) => {
            const data: ProgressiveResult = JSON.parse(event.data);

            setResults(prev => [...prev, data]);
            setCurrentTier(data.tier);

            if (data.tier === 1) {
                setStatus('CV extraction complete - enhancing...');
            } else if (data.tier === 2) {
                setStatus('Local AI enhancement complete - validating...');
            } else if (data.tier === 3) {
                setStatus(`Complete! ${(data.confidence * 100).toFixed(0)}% confidence`);
            }
        };

        ws.onclose = () => {
            setStatus('complete');
        };
    };

    return (
        <div className="progressive-extractor">
            <FileUpload onUpload={handleUpload} />

            <ProgressBar
                current={currentTier}
                total={3}
                status={status}
            />

            {results.map((result, i) => (
                <ResultCard key={i} result={result} />
            ))}
        </div>
    );
}
```

---

## Medium-Term (Next Sprint)

### 5. Result Caching üíæ

**Benefit**: 60-80% cost reduction on repeated extractions

```python
# extractors/extractors/ai/cache.py

import hashlib
import json
from typing import Dict, Any, Optional
import redis

class ResultCache:
    """Cache extraction results by image hash."""

    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis = redis.from_url(redis_url)
        self.ttl = 3600  # 1 hour

    def get(self, image_hash: str, tier: int) -> Optional[Dict[str, Any]]:
        """Get cached result for image hash + tier."""
        key = f"extract:{image_hash}:tier{tier}"
        cached = self.redis.get(key)

        if cached:
            return json.loads(cached)
        return None

    def set(self, image_hash: str, tier: int, result: Dict[str, Any]):
        """Cache result for image hash + tier."""
        key = f"extract:{image_hash}:tier{tier}"
        self.redis.setex(key, self.ttl, json.dumps(result))

    def image_hash(self, img: np.ndarray) -> str:
        """Calculate perceptual hash of image."""
        return hashlib.sha256(img.tobytes()).hexdigest()[:16]
```

---

### 6. Auto-Weighting ML Model üß†

**Benefit**: Learn optimal extractor weights from historical data

```python
# extractors/extractors/ai/auto_weight.py

import numpy as np
from sklearn.linear_model import LinearRegression

class AutoWeightOptimizer:
    """Learn optimal extractor weights from user feedback."""

    def __init__(self):
        self.model = LinearRegression()
        self.history = []

    def record_feedback(self, extractors: List[str], user_choice: str):
        """Record which extractor user chose as correct."""
        # Build training data
        # X: extractor outputs, y: user preference
        ...

    def optimize_weights(self) -> Dict[str, float]:
        """Train model and return optimized weights."""
        # Fit regression model
        # Return learned weights
        ...
```

---

### 7. Real-Time Dashboard üìä

**Benefit**: Monitor extractor performance, cost, confidence trends

```typescript
// frontend/src/pages/DashboardPage.tsx

export function ExtractorDashboard() {
    const [metrics, setMetrics] = useState(null);

    useEffect(() => {
        // Fetch metrics from backend
        fetch('/api/metrics/extractors')
            .then(res => res.json())
            .then(setMetrics);
    }, []);

    return (
        <div className="dashboard">
            <MetricsCard title="Average Confidence">
                {metrics?.avg_confidence.toFixed(2)}
            </MetricsCard>

            <MetricsCard title="Total Cost (30d)">
                ${metrics?.total_cost.toFixed(2)}
            </MetricsCard>

            <Chart
                data={metrics?.confidence_over_time}
                title="Confidence Trends"
            />

            <ExtractorTable
                extractors={metrics?.extractors}
            />
        </div>
    );
}
```

---

## Long-Term (v2.3+)

### 8. Adaptive Budgets üí∞

Automatically adjust budget based on image complexity:

```python
def estimate_complexity(img: np.ndarray) -> float:
    """Estimate image complexity (0-1)."""
    # More gradients = more complex
    # More unique colors = more complex
    # Higher resolution = more complex
    ...

def adaptive_budget(img: np.ndarray, base_budget: float = 0.10) -> float:
    """Calculate adaptive budget based on complexity."""
    complexity = estimate_complexity(img)
    return base_budget * (0.5 + complexity * 0.5)  # 50-100% of base
```

### 9. A/B Testing Framework üî¨

Compare extractor accuracy on labeled datasets:

```python
def ab_test_extractors(
    dataset: List[Tuple[np.ndarray, Dict[str, Any]]],
    extractors: List[ExtractorConfig]
) -> Dict[str, float]:
    """Test extractors against ground truth."""

    results = {}
    for config in extractors:
        accuracy = 0.0

        for img, ground_truth in dataset:
            prediction = config.extract_fn([img])
            accuracy += calculate_accuracy(prediction, ground_truth)

        results[config.name] = accuracy / len(dataset)

    return results
```

---

## Summary

**Immediate** (This Week):
1. ‚úÖ Test progressive extraction with example script
2. üß™ Add unit tests for multi-extractor
3. ü§ñ Integrate GPT-4 Vision API
4. üé® Build React progressive UI components

**Medium-Term** (Next Sprint):
5. üíæ Implement result caching (60-80% cost reduction)
6. üß† Add auto-weighting ML model
7. üìä Build real-time monitoring dashboard

**Long-Term** (v2.3+):
8. üí∞ Adaptive budget allocation
9. üî¨ A/B testing framework
10. üåê Multi-language support (i18n)

---

**Current Status**: Architecture complete, ready for testing and AI integration!
